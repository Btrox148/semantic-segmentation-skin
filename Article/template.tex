%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required
\begin{filecontents*}{example.eps}
%!PS-Adobe-3.0 EPSF-3.0
%%BoundingBox: 19 19 221 221
%%CreationDate: Mon Sep 29 1997
%%Creator: programmed by hand (JK)
%%EndComments

gsave
newpath
  20 20 moveto
  20 220 lineto
  220 220 lineto
  220 20 lineto
closepath
2 setlinewidth
gsave
  .4 setgray fill
grestore
stroke
grestore
\end{filecontents*}
%
\RequirePackage{fix-cm}
%
%\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
%\documentclass[smallextended]{svjour3}       % onecolumn (second format)
\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%



% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{blindtext}
\usepackage[utf8]{inputenc}
\usepackage{booktabs}
\usepackage{tikz}
    \usetikzlibrary{arrows, matrix, calc, shapes.geometric, shapes.misc,positioning}
\usepackage{adjustbox}
\usepackage{caption}
%  \captionsetup{width=.75\textwidth}
\usepackage{subfig}
\usepackage{xcolor}
\usepackage{url}
\usepackage{hyperref}

\let\proof\relax
\let\endproof\relax

\usepackage{amsfonts, amsmath, amsthm, amssymb}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage[numbers, sort, compress]{natbib}

\definecolor{red_matrix}{rgb}{0.6, 0, 0}
\definecolor{green_matrix}{rgb}{0, 0.8, 0}
\definecolor{blue_matrix}{rgb}{0, 0.25, 1}

\usepackage{pifont}% http://ctan.org/pkg/pifont
	\newcommand{\cmark}{\color{green_matrix}\ding{51}}%
  \newcommand{\xmark}{\color{red}\ding{53}}%


% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
% \journalname{myjournal}
%
\begin{document}

\title{Semantic Segmentation for Skin Melanoma Detection%\thanks{Grants or other notes
%about the article that should go on the front page should be
%placed here. General acknowledgments should be placed at the end of the article.}
}
\subtitle{A convolutional neural network implementation.}

%\titlerunning{Semantic Segmentation}        % if too long for running head

\author{Mario Alberto Flores Hern√°ndez}

%\authorrunning{Short form of author list} % if too long for running head

\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor


\maketitle

\begin{abstract}
  The approach described in this paper for the detection of the skin cancer melanoma is known as semantic segmentation. The semantic segmentation is a computer vision task where the different regions of an image are classified according to a specific category. The method used for the implementation of a semantic segmentation software was using the convolutional neural network, specifically a feature pyramid network architecture (\texttt{FPN}), to train a model which performs the task. With the convolutional neural networks is possible to train models that replicate the necessary transformations to obtain a targeted output, the only requirement is a rich enough database of samples of the input data and the target or the expected output in relation with that input, which in this case was a segmentation mask of the dermoscopic image.

\keywords{Neural Network \and Convolution \and Semantic Segmentation \and Classification}
% \PACS{PACS code1 \and PACS code2 \and more}
% \subclass{MSC code1 \and MSC code2 \and more}
\end{abstract}

\section{Introduction}
Skin is considered one of the largest organs in the human body, its function is to protect the internal organs and structures from the harsh environmental conditions such as temperature, radiation and bacteria. The skin also functions as a large sensorial interface that let us feel the environment conditions such as the temperature in the ambience and the texture of the objects.
The skin cancer known as melanoma is one of the deadliest types of cancer if not detected in its early stages, because of the quick spread to other organs caused by the metastasis effect. One way to reduce its mortality rate is by using automatic detection with computer vision technologies, such as the deep learning technology. With deep learning is possible to train models that recognize the presence of the melanoma tissue using a configuration of fine-tuned parameters during a process known as \emph{training} which compares the input of the model and the known output to that input, and then update the parameters to get the computed output closer to the known output for a later use in unknown truth data.

\subsection{General Objective}
The \emph{general objective} is to implement a convolutional neural network technology with the purpose of creating a dermoscopic computer vision tool to help medical staff to detect skin melanoma tissue in a automated or assisted way, and to contribute the cause of reducing skin cancer mortality rates.

\subsection{Specific Objective}
The \emph{specific objective} is determine the process involved to implement said technology, the requirements and considerations implicit in the deployment of a convolutional neural network architecture and the required transformations of the data to make that possible.

\begin{figure}[h]
  \centering
  \subfloat[Input]{\includegraphics[width=0.4\columnwidth]{Figuras/input_1.png}}\qquad
  \subfloat[Ground Truth]{\includegraphics[width=0.4\columnwidth]{Figuras/mask_1.png}}
  \caption{Example of data input/output.}
\end{figure}


\section{Background}
In this section are introduced the basic concepts an theories required to understand the following proposals. Starting with some basic background aspects about the melanoma cancer, then continuing with the loss functions and the optimizer functions.

\subsection{Melanoma}
The skin cancer is a type of cell mutation that usually is related to the spontaneous appearance of asymmetric moles or patches of dark/red skin, bulges or scales on the skin surface. This mutation of the melanocytes (the cells responsible for the skin pigmentation) can affect any skin tone, mostly exposed parts of the body to ultra-violet light like the arms and the neck, but not exclusively.

\begin{figure}[h]
  \centering
  \hspace{-0.4cm}
  \subfloat[]{\includegraphics[width=0.3\columnwidth]{Figuras/mela_1.jpg}}
  \subfloat[]{\includegraphics[width=0.3\columnwidth]{Figuras/mela_2.jpg}}
  \subfloat[]{\includegraphics[width=0.3\columnwidth]{Figuras/mela_3.jpg}}
  \caption{Examples of skin melanoma obtained from the ISIC \citep{isic_skin} database.}
\end{figure}

There are two main factors involved in the mutation of the melanocytes cells: intrinsic aging and extrinsic factors. The \emph{intrinsic} aging occurs inevitably as a consequence of the time, while the \emph{extrinsic} factors are rather external like the sunlight, nicotine abuse unhealthy lifestyles \citep{farage2008intrinsic}.

\subsection{Image Dimensions}
An important aspect to consider is the dimension of the images before introducing them to the neural network architecture. Images in a gray-scale configuration do not share the same dimensions of an image of the same resolution in a \emph{rgb} configuration. A gray-scale or mono-channel image resolution can be described as $m \times n$, where $m$ and $n$ are the width and height respectively. This however is not the case for the rgb configuration which dimensions are described as $m \times n \times k$, where $k$ is the number of color channels. That means that in color images there are a specific number of layers where each has an independent set of pixel intensity values, as described in figure \ref{fig:img_dims}.


\begin{figure}[h]
  \centering
  \resizebox{0.4\columnwidth}{!}{%
  \subfloat[Grayscale.]{
      \label{mtr_1}
      \begin{tikzpicture}
          \draw[step=5mm, black, line width=0.30mm] (-2,-2) grid (2,2);
          \node at (0,-2.5) {\Large$\text{dim} = m \times n$};
      \end{tikzpicture}%
  }
  }
  \qquad
  \subfloat[Color.]{
      \label{mtr_2}
      \resizebox{0.45\columnwidth}{!}{% 
      \begin{tikzpicture}
          \draw[step=5mm, red_matrix, line width=0.30mm] (-2.5,-2.5) grid (1.5,1.5);
          \draw[step=5mm, green_matrix, line width=0.30mm] (-2,-2) grid (2,2);
          \draw[step=5mm, blue, line width=0.30mm] (-1.5,-1.5) grid (2.5,2.5);
          \node at (0, -3) {\Large$\text{dim} = m \times n \times k$};
      \end{tikzpicture}%
      }
  }
  \caption{Dimensional comparison between images.}
  \label{fig:img_dims}
\end{figure}

\subsection{Convolutional Neural Networks}
A convolutional neural network is a configuration of layers of \emph{convolution}.


\begin{figure}[h]
  \centering
  \resizebox{\columnwidth}{!}{%
  \begin{tikzpicture}
		\node at (0.5,-1){\begin{tabular}{c}input image\\layer $l = 0$\end{tabular}};
		
		\draw (0,0) -- (1,0) -- (1,1) -- (0,1) -- (0,0);
		
		\node at (3,3.5){\begin{tabular}{c}convolutional layer\\with non-linearities\\layer $l = 1$\end{tabular}};
		
		\draw[fill=black,opacity=0.2,draw=black] (2.75,1.25) -- (3.75,1.25) -- (3.75,2.25) -- (2.75,2.25) -- (2.75,1.25);
		\draw[fill=black,opacity=0.2,draw=black] (2.5,1) -- (3.5,1) -- (3.5,2) -- (2.5,2) -- (2.5,1);
		\draw[fill=black,opacity=0.2,draw=black] (2.25,0.75) -- (3.25,0.75) -- (3.25,1.75) -- (2.25,1.75) -- (2.25,0.75);
		\draw[fill=black,opacity=0.2,draw=black] (2,0.5) -- (3,0.5) -- (3,1.5) -- (2,1.5) -- (2,0.5);
		\draw[fill=black,opacity=0.2,draw=black] (1.75,0.25) -- (2.75,0.25) -- (2.75,1.25) -- (1.75,1.25) -- (1.75,0.25);
		\draw[fill=black,opacity=0.2,draw=black] (1.5,0) -- (2.5,0) -- (2.5,1) -- (1.5,1) -- (1.5,0);
		
		\node at (4.5,-1){\begin{tabular}{c}subsampling layer\\layer $l = 3$\end{tabular}};
		
		\draw[fill=black,opacity=0.2,draw=black] (5,1.25) -- (5.75,1.25) -- (5.75,2) -- (5,2) -- (5,1.25);
		\draw[fill=black,opacity=0.2,draw=black] (4.75,1) -- (5.5,1) -- (5.5,1.75) -- (4.75,1.75) -- (4.75,1);
		\draw[fill=black,opacity=0.2,draw=black] (4.5,0.75) -- (5.25,0.75) -- (5.25,1.5) -- (4.5,1.5) -- (4.5,0.75);
		\draw[fill=black,opacity=0.2,draw=black] (4.25,0.5) -- (5,0.5) -- (5,1.25) -- (4.25,1.25) -- (4.25,0.5);
		\draw[fill=black,opacity=0.2,draw=black] (4,0.25) -- (4.75,0.25) -- (4.75,1) -- (4,1) -- (4,0.25);
		\draw[fill=black,opacity=0.2,draw=black] (3.75,0) -- (4.5,0) -- (4.5,0.75) -- (3.75,0.75) -- (3.75,0);
		
		\node at (7,3.5){\begin{tabular}{c}convolutional layer\\with non-linearities\\layer $l = 4$\end{tabular}};
		
		\draw[fill=black,opacity=0.2,draw=black] (7.5,1.75) -- (8.25,1.75) -- (8.25,2.5) -- (7.5,2.5) -- (7.5,1.75);
		\draw[fill=black,opacity=0.2,draw=black] (7.25,1.5) -- (8,1.5) -- (8,2.25) -- (7.25,2.25) -- (7.25,1.5);
		\draw[fill=black,opacity=0.2,draw=black] (7,1.25) -- (7.75,1.25) -- (7.75,2) -- (7,2) -- (7,1.25);
		\draw[fill=black,opacity=0.2,draw=black] (6.75,1) -- (7.5,1) -- (7.5,1.75) -- (6.75,1.75) -- (6.75,1);
		\draw[fill=black,opacity=0.2,draw=black] (6.5,0.75) -- (7.25,0.75) -- (7.25,1.5) -- (6.5,1.5) -- (6.5,0.75);
		\draw[fill=black,opacity=0.2,draw=black] (6.25,0.5) -- (7,0.5) -- (7,1.25) -- (6.25,1.25) -- (6.25,0.5);
		\draw[fill=black,opacity=0.2,draw=black] (6,0.25) -- (6.75,0.25) -- (6.75,1) -- (6,1) -- (6,0.25);
		\draw[fill=black,opacity=0.2,draw=black] (5.75,0) -- (6.5,0) -- (6.5,0.75) -- (5.75,0.75) -- (5.75,0);
		
		\node at (9.5,-1){\begin{tabular}{c}subsampling layer\\layer $l = 6$\end{tabular}};
		
		\draw[fill=black,opacity=0.2,draw=black] (10,1.75) -- (10.5,1.75) -- (10.5,2.25) -- (10,2.25) -- (10,1.75);
		\draw[fill=black,opacity=0.2,draw=black] (9.75,1.5) -- (10.25,1.5) -- (10.25,2) -- (9.75,2) -- (9.75,1.5);
		\draw[fill=black,opacity=0.2,draw=black] (9.5,1.25) -- (10,1.25) -- (10,1.75) -- (9.5,1.75) -- (9.5,1.25);
		\draw[fill=black,opacity=0.2,draw=black] (9.25,1) -- (9.75,1) -- (9.75,1.5) -- (9.25,1.5) -- (9.25,1);
		\draw[fill=black,opacity=0.2,draw=black] (9,0.75) -- (9.5,0.75) -- (9.5,1.25) -- (9,1.25) -- (9,0.75);
		\draw[fill=black,opacity=0.2,draw=black] (8.75,0.5) -- (9.25,0.5) -- (9.25,1) -- (8.75,1) -- (8.75,0.5);
		\draw[fill=black,opacity=0.2,draw=black] (8.5,0.25) -- (9,0.25) -- (9,0.75) -- (8.5,0.75) -- (8.5,0.25);
		\draw[fill=black,opacity=0.2,draw=black] (8.25,0) -- (8.75,0) -- (8.75,0.5) -- (8.25,0.5) -- (8.25,0);
		
		\node at (12,3.5){\begin{tabular}{c}fully connected layer\\layer $l = 7$\end{tabular}};
		
		\draw[fill=black,draw=black,opacity=0.5] (10.5,0) -- (11,0) -- (12.5,1.75) -- (12,1.75) -- (10.5,0);
		
		\node at (13,-1){\begin{tabular}{c}fully connected layer\\output layer $l = 8$\end{tabular}};
		
		\draw[fill=black,draw=black,opacity=0.5] (12.5,0.5) -- (13,0.5) -- (13.65,1.25) -- (13.15,1.25) -- (12.5,0.5);
  \end{tikzpicture}%
  }
  \caption{Convolutional neural network for classification, figure from David Stutz \citep{Stutz:2017}.}
\end{figure}

\section{Related Work}
In this section the reviewed literature was summarized to make a comparison between different methods to approach the melanoma detection problem and same approaches for the solution of different problems to get a scope of the opportunity areas, the advantages and disadvantages of the proposed solution.

\subsection{Comparative Analysis}

These are the characteristics used to compare the implementation in this paper with the work from the other authors.
\begin{description}
  \item[\textbf{Classification.}] Has the capacity of classify between two or more categories.
  \item[\textbf{Segmentation.}] Can recognize the pixel-wise region of the different categories.
  \item[\textbf{Supervised.}] Requires a curated database to perform the training process.
  \item[\textbf{Pre-trained.}] Can initialize the model with pre-trained weights.
  \item[\textbf{Evaluation.}] Can determine the precision of the computed output in comparison with the ground truth.     
\end{description}

\begin{table}[h]
  \begin{center}
  \caption{Similitude and difference in the literature reviewed; the available features are shown as {\cmark}, and the not available are represented with {\xmark}.}
  \hspace{-0.5cm}
  \scalebox{0.75}{
      \begin{tabular}[t]{|l|l|l|l|l|l|l|l|}
          \hline
          \bf{Work} & \bf{Model} & \bf{\rotatebox{90}{Classification}} & \bf{\rotatebox{90}{Segmentation\phantom{m}}} & \bf{\rotatebox{90}{Supervised}} & \bf{\rotatebox{90}{Pre-trained\phantom{m}}} & \bf{\rotatebox{90}{Evaluation}} & \bf{Output}\\
          \hline
          \citet{DBLP:journals/corr/BadrinarayananK15} & \texttt{SegNet} & \cmark & \cmark & \cmark & \cmark & \cmark & label\\
          \citet{DBLP:journals/corr/RonnebergerFB15} & \texttt{U-net} & \cmark & \cmark & \cmark & \xmark & \cmark &label map\\
          \citet{DBLP:journals/corr/ChenPK0Y16} & \texttt{DeepLab} & \cmark & \cmark & \cmark & \xmark & \cmark &label map\\   
          \citet{DBLP:journals/corr/TeichmannWZCU16} & \texttt{MultiNet} & \cmark & \cmark & \cmark & \xmark & \cmark &label map\\   
          \citet{KRONER2020261} & \texttt{VGG16} & \cmark & \cmark & \cmark & \cmark & \cmark & heat map\\ 
          \citet{KADAMPUR2020100282} & \texttt{CNN} & \cmark & \xmark & \cmark & \xmark & \cmark &label map\\    
          \citet{zhou2019emerging} & \texttt{ML / SVM} & \cmark & \xmark & \cmark & \xmark & \cmark & label\\    
          \citet{DBLP:journals/corr/LucCCV16} & \texttt{CNN/GAN} & \cmark & \cmark & \cmark & \cmark & \cmark &label map\\         
          \citet{JAIN2015735} & \texttt{A.B.C.D} & \xmark & \cmark & \xmark & \xmark & \xmark & label\\
          \hline
          Proposal & \texttt{FPN} & \cmark & \cmark & \cmark & \cmark & \cmark &label map\\
          \hline   
      \end{tabular}
      }
  \end{center}
  \label{Tab:comp_1}
\end{table}

\subsection{Opportunity Area}
For the semantic segmentation task is indispensable to count with a coder and decoder which dimensions and output coincide with the requirements of the input data and the desired output. The neural networks in contrast with other technologies offer that versatility, they can adjust the parameters during the transformation process to obtain the desired output in the dimensional aspect.  

\section{Proposed Solution}
The approach proposed in this paper for the automated detection of the skin melanoma is the training of a model using a \texttt{FPN} convolutional neural network architecture, which output would be the probabilistic map of the regions inside of the dermoscopic input image. The language selected for the implementation of said neural network architecture was \texttt{Python}, because is a powerful, dynamic and quick to deploy language which also contains the \texttt{Torch} library which is the core tool of the implementation.

\section{Experiments}
The methods used to evaluate the performance of the training process and also gauge the probabilistic map predictions are described in this section. Also the analysis of the threshold is included here.

\subsection{Threshold Analysis}
The threshold analysis consists of the visualization of the intensity bands where the pixels tend to cluster and that can affect the complexity level of the computation in the model.


\subsection{Dice Loss}
To determine the probability that two pixel intensity maps are similar is required to use an evaluation function which determines that similitude. The dice loss function compares the margin of equal values along two matrices and then divides it by the number of total pixels between both maps. The result obtained is a coefficient between zero and one, which represents the error.

  \begin{equation}\label{eq:dice_loss}
      \text{DL} = \frac{2|A \cap B |}{|A| + |B|}
  \end{equation}        


\subsection{Results}

\begin{figure}[h]
  \centering
  \subfloat[Epoch 1]{\includegraphics[width=0.3\columnwidth]{../Plots/Eng/dl_epoch_0.png}}
  \subfloat[Epoch 2]{\includegraphics[width=0.3\columnwidth]{../Plots/Eng/dl_epoch_1.png}}
  \subfloat[Epoch 3]{\includegraphics[width=0.3\columnwidth]{../Plots/Eng/dl_epoch_2.png}} \\
  \subfloat[Epoch 4]{\includegraphics[width=0.3\columnwidth]{../Plots/Eng/dl_epoch_3.png}}
  \subfloat[Epoch 5]{\includegraphics[width=0.3\columnwidth]{../Plots/Eng/dl_epoch_4.png}}
  \subfloat[Epoch 6]{\includegraphics[width=0.3\columnwidth]{../Plots/Eng/dl_epoch_5.png}}
  \caption{Error reduction in the training process by iteration.}
  \label{fig:DL_plots}
\end{figure}


\subsection{Intersection Over Union}
Similar to the \emph{dice loss} function, the \emph{intersection over union} or \emph{Jaccard equation} compares the similar pixel intensities between two binary maps with the difference that the error has more penalization, which means that the result obtained is more robust. This is useful in the \emph{validation} process where the error has more significance.

\begin{equation}\label{eq:jacc}
  \text{IoU} = \frac{|A \cap B|}{| A \cup B |} = \frac{|A \cap B|}{|A| + |B| - |A \cap B|} \text{,}
\end{equation}

\subsection{Results}

\begin{figure}[h]
  \centering
  \subfloat[Epoch 1]{\includegraphics[width=0.3\columnwidth]{../Plots/Eng/iou_epoch_0.png}}
  \subfloat[Epoch 2]{\includegraphics[width=0.3\columnwidth]{../Plots/Eng/iou_epoch_1.png}}
  \subfloat[Epoch 3]{\includegraphics[width=0.3\columnwidth]{../Plots/Eng/iou_epoch_2.png}} \\
  \subfloat[Epoch 4]{\includegraphics[width=0.3\columnwidth]{../Plots/Eng/iou_epoch_3.png}}
  \subfloat[Epoch 5]{\includegraphics[width=0.3\columnwidth]{../Plots/Eng/iou_epoch_4.png}}
  \subfloat[Epoch 6]{\includegraphics[width=0.3\columnwidth]{../Plots/Eng/iou_epoch_5.png}}
  \caption{Similitude between ground truth and computed output according to the IoU score by iteration.}
  \label{fig:IoU_plots}
\end{figure}

\subsection{Discussion}

The process of error reduction is not linear, the figures \ref{fig:DL_plots} and \ref{fig:IoU_plots} show that exists some fluctuation between iterations, this however does not represent a problem because the code automatically saves the highest score model on each epoch. In the table \ref{table:summary}, the highest score by epoch was registered.

\begin{table}[b]
  \centering
  \caption{Summarized results by epoch.}
  \begin{tabular}{c c c}
    \toprule
    \textbf{Epoch} & \textbf{DL} & \textbf{IoU} \\
    \midrule
    1 & 0.2968 & 0.5616 \\
    2 & 0.2972 & 0.5546 \\
    3 & 0.2690 & 0.5879 \\
    4 & 0.2960 & 0.6389 \\
    5 & 0.2371 & 0.8415 \\
    6 & 0.0929 & 0.8626 \\
    7 & 0.0792 & 0.8626 \\
    8 & 0.0692 & 0.8798 \\
    9 & 0.0556 & 0.9017 \\
    10 & 0.0542 & 0.9039 \\
    \bottomrule
    
  \end{tabular}
  \label{table:summary}
\end{table}

\section{Conclusion}
In this document was described the process of the implementation of a convolutional neural network for the semantic segmentation task, by which is possible to classify between the health and melanoma tissue regions inside the dermoscopic images. A feature pyramid network was implemented and as a result of the convolutional layers and the associated weights(parameters) were adjust in relationship with the evaluation criteria from the dice loss and Jaccard functions. The result was a probabilistic map of the different categories inside the image.

\subsection{Discussion}
The main objective of this implementation was to obtain a tool able to perform perform the segmentation task in the dermoscopic images that hopefully contributes to the cause of the early detection of the melanoma disease. Thanks to the pre-build architectures of convolutional neural networks from the library provided by \citet{Yakubovskiy:2019}, it was possible to quickly deploy the FPN architecture and configure the network for the pre-processing and the training.

\subsection{Future Work}
This implementation of the semantic segmentation task has one major limitation, which is that it can distinguish between different categories inside the regions of the dermoscopic images, but it cannot distinguish between two objects of the same category which means that for example this same two objects are considered one single object inside the same category. This limitation in particular doesn't affect the purpose of this implementation because in this case the melanoma will always represent one single instance in the person. There are a lot of papers which talk about instanced segmentation where this is not the case, and the different instances of the different objects under the same category are recognized as different instances. I would like to experiment with instanciation and multi-class segmentation in the future. 

\section{GitHub Repository}

The full source code and the data obtained from the experiments can be consulted by scanning the following \texttt{QR code}.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.65\columnwidth]{repoqr.eps}
\end{figure}
\hspace{-1cm}
\url{https://github.com/Btrox148/semantic-segmentation-skin}

%\begin{acknowledgements}
%If you'd like to thank anyone, place your comments here
%and remove the percent signs.
%\end{acknowledgements}


% Authors must disclose all relationships or interests that 
% could have direct or potential influence or impart bias on 
% the work: 
%
% \section*{Conflict of interest}
%
% The authors declare that they have no conflict of interest.
\section{References}

% BibTeX users please use one of
%\bibliographystyle{spbasic}      % basic style, author-year citations
%\bibliographystyle{spmpsci}      % mathematics and physical sciences
%\bibliographystyle{spphys}       % APS-like style for physics

\renewcommand\refname{\vskip -1cm}
\bibliographystyle{mighelnat}    % FIME - Custom Style.

\footnotesize
\bibliography{MiBiblio}   % name your BibTeX data base

% Non-BibTeX users please use
%\begin{thebibliography}{}
%
% and use \bibitem to create references. Consult the Instructions
% for authors for reference list style.
%
%\bibitem{RefJ}
% Format for Journal Reference
%Author, Article title, Journal, Volume, page numbers (year)
% Format for books
%\bibitem{RefB}
%Author, Book title, page numbers. Publisher, place (year)
% etc
%\end{thebibliography}

\end{document}
% end of file template.tex

